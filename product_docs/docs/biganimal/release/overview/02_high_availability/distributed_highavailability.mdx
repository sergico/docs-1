---
title: "Distributed high availability"
---

Distributed high-availability clusters are powered by [EDB Postgres Distributed](/pgd/latest/) using multi-master logical replication to deliver more advanced cluster management compared to a physical replication-based system. Distributed high-availability clusters offer the ability to deploy a cluster across multiple regions or a single region. For use cases where high availability across regions is a major concern, a cluster deployment with distributed high availability enabled can provide one region with three data nodes, another region with the same configuration, and one group with a witness node in a third region for a true active-active solution. 

Distributed high-availability clusters support both EDB Postgres Advanced Server and EDB Postgres Extended Server database distributions. 

Distributed high-availability clusters contain one or two data groups. Your data groups can contain either three data nodes or two data nodes and one witness node. One of these data nodes is the leader at any given time, while the rest are shadow nodes. We recommend that you don't use two data nodes and one witness node in production unless you use asynchronous [commit scopes](/pgd/latest/durability/commit-scopes/). 

[PGD Proxy](/pgd/latest/routing/proxy) routes all application traffic to the leader node, which acts as the principal write target to reduce the potential for data conflicts. PGD Proxy leverages a distributed consensus model to determine availability of the data nodes in the cluster. On failure or unavailability of the leader, PGD Proxy elects a new leader and redirects application traffic. Together with the core capabilities of EDB Postgres Distributed, this mechanism of routing application traffic to the leader node enables fast failover and switchover.

The witness node/witness group doesn't host data but exists for management purposes, supporting operations that require a consensus, for example, in case of an availability zone failure. 

!!!Note
   Operations against a distributed high-availability cluster leverage the [EDB Postgres Distributed switchover](/pgd/latest/cli/command_ref/pgd_switchover/) feature which provides sub-second interruptions during planned lifecycle operations.

## Single data location 

A single data location configuration has one data group and either:

- two data nodes with one lead and one shadow, and a witness node each in separate availability zones

    ![region(2 data + 1 witness)](../images/image5.png)

- three data nodes with one lead and two shadow nodes each in separate availability zones

    ![region(3 data)](../images/image3.png)

## Multiple data locations and witness node

A multiple data location configuration has two data groups that contain either:

- Two data nodes (not recommended for production)

  - A data node, shadow node, and a witness node in one region
  - The same configuration in another region
  - A witness node in a third region

    ![region(2 data + 1 shadow) + region(2 data + 1 shadow) + region(1 witness)](../images/2dn-1wn-2dn-1wn-1wg.png)

- Three data nodes

  - A data node and two shadow nodes in one region
  - The same configuration in another region
  - A witness node in a third region

    ![region(2 data + 1 shadow) + region(2 data + 1 shadow) + region(1 witness)](../images/eha.png)


### Cross-Cloud Service Providers(CSP) witness node

By default, the cloud service provider selected for the data groups is pre-selected for the witness node.

To further improve availability, siting a witness node on a different cloud service provider can enable the witness to be available to the cluster when there are only two regions available to the cluster on one cloud service provider.

Users can change the cloud service provider of the witness node to be one of the other two cloud service provider.

Cross-cloud service provider witness node are available with AWS, Azure and GCP, using your own cloud account and BigAnimal's cloud account. It is enabled by default and applies to both multi-region configurations available with PGD. The database price is not charged for witness groups, it is charged for the infrastructure it uses. The cost of the cross-CSP witness node is reflected in the pricing estimate footer.


## For more information

For instructions on creating a distributed high-availability cluster using the BigAnimal portal, see [Creating a distributed high-availability cluster](../getting_started/creating_a_cluster/creating_an_eha_cluster/).

For instructions on creating, retrieving information from, and managing a distributed high-availability cluster using the BigAnimal CLI, see [Using the BigAnimal CLI](/biganimal/latest/reference/cli/managing_clusters/#managing-distributed-high-availability-clusters).
